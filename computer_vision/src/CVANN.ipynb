{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# CVANN - Computer Vision Artificial Neural Network\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import MaxPooling2D, Convolution2D\n",
    "from keras.layers import Dense, Activation, Flatten, Dropout\n",
    "from keras.utils.np_utils import to_categorical\n",
    "\n",
    "from keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def progress_bar(max_, bar_width=40):\n",
    "    try:\n",
    "        if not progress_bar.counter:\n",
    "            progress_bar.counter = 0\n",
    "\n",
    "    except AttributeError:\n",
    "        progress_bar.counter = 0\n",
    "\n",
    "    percentage = str(int((progress_bar.counter + 1)/max_*100))\n",
    "    bar = '[' + '-'*(int(bar_width*(progress_bar.counter+1)/max_) - 1) + '>' + \\\n",
    "          '.'*(bar_width - int(bar_width*(progress_bar.counter + 1)/max_)) + ']' + percentage + '%'\n",
    "\n",
    "    progress_bar.counter += 1\n",
    "\n",
    "    if progress_bar.counter == 1:\n",
    "        print(bar, end='', flush=True)\n",
    "    else:\n",
    "        print('\\b' * len(bar) + bar, end='', flush=True)\n",
    "\n",
    "    if progress_bar.counter == max_:\n",
    "        progress_bar.counter = 0\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sep_grayscale_intervals(img, num_intervals=4, output_path=None):\n",
    "\n",
    "    img_array = img_to_array(img)\n",
    "\n",
    "    for img in range(len(img_array)):\n",
    "        for i in range(len(img_array[img])):\n",
    "            for j in range(len(img_array[img][i])):\n",
    "\n",
    "                pixel = img_array[img][i][j]\n",
    "                new_value = pixel\n",
    "\n",
    "                for interval in range(num_intervals):\n",
    "                    interval_max = 255-((256/num_intervals)*interval)\n",
    "                    if pixel < interval_max:\n",
    "                        new_value = interval_max-(256/num_intervals)\n",
    "\n",
    "                img_array[img][i][j] = new_value\n",
    "\n",
    "    if output_path:\n",
    "        img.save(output_path, 'JPEG')\n",
    "\n",
    "    return array_to_img(img_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_data(img_dir_name, img_dimensions=(120, 90), verbose=0):\n",
    "\n",
    "    print('\\nLoading data: \"' + str(img_dir_name) + '\"')\n",
    "\n",
    "    directory = '..' + os.sep + 'resources' + os.sep + str(img_dir_name)\n",
    "    sub_dir_names = [file for file in os.listdir(directory) if not file.startswith('.')]\n",
    "\n",
    "    # calculate the total number of images | initialize the dictionary mapping the names to integers\n",
    "    total_num_images = 0\n",
    "    i = 0\n",
    "    dictionary = {}\n",
    "\n",
    "    for sub_dir in sub_dir_names:\n",
    "        sub_dir_path = directory + os.sep + sub_dir\n",
    "        image_files = [file for file in os.listdir(sub_dir_path) if not file.startswith('.')]\n",
    "        total_num_images += len(image_files)\n",
    "\n",
    "        dictionary[sub_dir] = i\n",
    "        i += 1\n",
    "\n",
    "    print('Loading ' + str(total_num_images) + ' images\\n')\n",
    "\n",
    "    data = []\n",
    "    labels = np.zeros((total_num_images, 1)).astype('int')\n",
    "    i = 0\n",
    "    for sub_dir in sub_dir_names:\n",
    "        if verbose == 1:\n",
    "            end = ' '\n",
    "        else:\n",
    "            end = '\\n'\n",
    "        print('current directory of images: ' + sub_dir, end=end)\n",
    "\n",
    "        label = int(dictionary[sub_dir])  # the 'label' associated to this folder\n",
    "        sub_dir_path = directory + os.sep + sub_dir\n",
    "        img_files = [file for file in os.listdir(sub_dir_path) if not file.startswith('.')]\n",
    "\n",
    "        for file in img_files:\n",
    "            if verbose == 1:\n",
    "                progress_bar(len(img_files))  # testing phase\n",
    "            file_path = sub_dir_path + os.sep + file\n",
    "            img = load_img(file_path, grayscale=True, target_size=img_dimensions)\n",
    "            # img = sep_grayscale_intervals(img, num_intervals=8)  # testing phase\n",
    "            img_array = img_to_array(img)\n",
    "\n",
    "            labels = np.insert(labels, i, label, axis=0)  # insert the label at position i\n",
    "            labels = np.delete(labels, -1, axis=0)  # remove the one too many element\n",
    "            data.append(img_array)\n",
    "            i += 1\n",
    "\n",
    "    data = np.asarray(data)\n",
    "    data /= 255\n",
    "    nb_classes = len(dictionary)\n",
    "    labels = to_categorical(labels, nb_classes=nb_classes)\n",
    "\n",
    "    return data, labels, dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def neural_network(train, validation, nb_epoch=5, batch_size=40, activations=('relu', 'relu', 'softmax'),\n",
    "                   nb_filters=30, img_dimensions=(120, 90), pool_dimensions=(8, 6), conv_dimensions=(12, 9),\n",
    "                   optimizer='rmsprop'):\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(Convolution2D(nb_filter=nb_filters, nb_row=conv_dimensions[0], nb_col=conv_dimensions[1],\n",
    "                            input_shape=(1, img_dimensions[0], img_dimensions[1]), activation=activations[0]))\n",
    "\n",
    "    model.add(MaxPooling2D(pool_size=pool_dimensions))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Flatten())  # converts our 2D feature maps to 1D feature vectors\n",
    "    model.add(Dense(128, activation=activations[1]))\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Dense(train[2], activation=activations[2]))\n",
    "\n",
    "    model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    model.fit(x=train[0], y=train[1], nb_epoch=nb_epoch, batch_size=batch_size)\n",
    "\n",
    "    print('\\n')\n",
    "    loss, accuracy = model.evaluate(x=validation[0], y=validation[1], batch_size=batch_size, verbose=1)\n",
    "    print('loss: ' + str(round(loss, 4)))\n",
    "    print('accuracy: ' + str(round(accuracy, 4)) + '\\n')\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading data: \"OCRANN_train\"\n",
      "Loading 3100 images\n",
      "\n",
      "current directory of images: Sample001\n",
      "current directory of images: Sample002\n",
      "current directory of images: Sample003\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-51e12c3f3745>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'OCRANN_train'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# AV_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mvalidation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'OCRANN_validation'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-14-e206959ffb9c>\u001b[0m in \u001b[0;36mload_data\u001b[0;34m(img_dir_name, img_dimensions, verbose)\u001b[0m\n\u001b[1;32m     39\u001b[0m                 \u001b[0mprogress_bar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_files\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# testing phase\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m             \u001b[0mfile_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msub_dir_path\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msep\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m             \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_img\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrayscale\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mimg_dimensions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m             \u001b[0;31m# img = sep_grayscale_intervals(img, num_intervals=8)  # testing phase\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m             \u001b[0mimg_array\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimg_to_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/valerio/anaconda/lib/python3.5/site-packages/keras/preprocessing/image.py\u001b[0m in \u001b[0;36mload_img\u001b[0;34m(path, grayscale, target_size)\u001b[0m\n\u001b[1;32m    173\u001b[0m     \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mgrayscale\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 175\u001b[0;31m         \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'L'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    176\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Ensure 3 channel even when loaded image is grayscale\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m         \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'RGB'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/valerio/anaconda/lib/python3.5/site-packages/PIL/Image.py\u001b[0m in \u001b[0;36mconvert\u001b[0;34m(self, mode, matrix, dither, palette, colors)\u001b[0m\n\u001b[1;32m    844\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    845\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 846\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    847\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    848\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmatrix\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/valerio/anaconda/lib/python3.5/site-packages/PIL/ImageFile.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    219\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m                     \u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mb\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 221\u001b[0;31m                     \u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    222\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mn\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m                         \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train = load_data('OCRANN_train', verbose=0)  # AV_\n",
    "validation = load_data('OCRANN_validation', verbose=0)  # verbose/progress bar only works in the command line..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = neural_network(train, validation, activations=('relu', 'relu', 'softmax'), nb_epoch=1, batch_size=30, optimizer='rmsprop')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n",
      "9145/9145 [==============================] - 474s - loss: 2.3606 - acc: 0.4742   \n",
      "Epoch 2/4\n",
      "9145/9145 [==============================] - 435s - loss: 2.2913 - acc: 0.4866   \n",
      "Epoch 3/4\n",
      "9145/9145 [==============================] - 436s - loss: 2.2137 - acc: 0.4975   \n",
      "Epoch 4/4\n",
      "9145/9145 [==============================] - 434s - loss: 2.1935 - acc: 0.5088   \n",
      "\n",
      "\n",
      "9145/9145 [==============================] - 273s   \n",
      "loss: 1.7076\n",
      "accuracy: 0.6232\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model.fit(x=train[0], y=train[1], nb_epoch=4, batch_size=10)\n",
    "print('\\n')\n",
    "loss, accuracy = model.evaluate(x=validation[0], y=validation[1], batch_size=1, verbose=1)\n",
    "print('loss: ' + str(round(loss, 4)))\n",
    "print('accuracy: ' + str(round(accuracy, 4)) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
